package rl

import (
	"fmt"
	"math"
	"math/rand"
	"sync"
	"time"
)

// State –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è RL
type State struct {
	Vectors    [3][3]int // –í–µ–∫—Ç–æ—Ä—ã –¥–Ω—è/–º–µ—Å—è—Ü–∞/–≥–æ–¥–∞
	Chakras    []int     // –ê–∫—Ç–∏–≤–Ω—ã–µ —á–∞–∫—Ä—ã (0-6)
	Symptoms   []string  // –í—ã–±—Ä–∞–Ω–Ω—ã–µ —Å–∏–º–ø—Ç–æ–º—ã
	History    []Action  // –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π
}

// Action ‚Äî –¥–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç–∞ (–∫–∞–∫—É—é –∞—Ñ—Ñ–∏—Ä–º–∞—Ü–∏—é –ø–æ–∫–∞–∑–∞—Ç—å)
type Action struct {
	ID          string
	Text        string
	Author      string
	ChakraIndex int
	Reason      string // –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∞ (–¥–ª—è –¥–æ–≤–µ—Ä–∏—è)
}

// Reward ‚Äî —Ç–∏–ø—ã –Ω–∞–≥—Ä–∞–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
type Reward float64

const (
	RewardCopied   Reward = 1.0 // –ö–ª–∏–µ–Ω—Ç —Å–∫–æ–ø–∏—Ä–æ–≤–∞–ª —Ç–µ–∫—Å—Ç
	RewardPrinted  Reward = 2.0 // –ö–ª–∏–µ–Ω—Ç —Ä–∞—Å–ø–µ—á–∞—Ç–∞–ª
	RewardReturned Reward = 5.0 // –ö–ª–∏–µ–Ω—Ç –≤–µ—Ä–Ω—É–ª—Å—è —á–µ—Ä–µ–∑ –Ω–µ–¥–µ–ª—é
	RewardIgnored  Reward = -1.0// –ö–ª–∏–µ–Ω—Ç –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–ª
)

// Agent ‚Äî Q-learning –∞–≥–µ–Ω—Ç
type Agent struct {
	qTable       map[string]map[string]float64 // stateKey -> actionID -> Q-value
	mu           sync.RWMutex
	learningRate float64
	discount     float64
	exploration  float64 // epsilon-greedy
}

// NewAgent —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
func NewAgent(lr, discount, exploration float64) *Agent {
	return &Agent{
		qTable:       make(map[string]map[string]float64),
		learningRate: lr,
		discount:     discount,
		exploration:  exploration,
	}
}

// stateKey –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤—ã–π –∫–ª—é—á –∏–∑ State (–¥–ª—è —Ç–∞–±–ª–∏—Ü—ã Q)
func stateKey(s State) string {
	// –£–ø—Ä–æ—â—ë–Ω–Ω–æ: —Ö—ç—à–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã + —á–∞–∫—Ä—ã
	return fmt.Sprintf("%v_%v", s.Vectors, s.Chakras)
}

// ChooseAction –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ (epsilon-greedy + –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ)
func (a *Agent) ChooseAction(s State, available []Action) Action {
	a.mu.RLock()
	defer a.mu.RUnlock()

	key := stateKey(s)
	
	// –° –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é exploration ‚Äî —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ)
	if rand.Float64() < a.exploration {
		act := available[rand.Intn(len(available))]
		act.Reason = "üé≤ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞"
		return act
	}

	// –ò–Ω–∞—á–µ ‚Äî –ª—É—á—à–µ–µ –∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (—ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è)
	bestQ := -math.MaxFloat64
	var best Action
	for _, act := range available {
		q := a.qTable[key][act.ID]
		if q > bestQ {
			bestQ = q
			best = act
		}
	}
	
	if bestQ > 0 {
		best.Reason = fmt.Sprintf("‚úÖ –£—Å–ø–µ—à–Ω–æ –≤ %d –ø–æ—Ö–æ–∂–∏—Ö —Å–ª—É—á–∞—è—Ö", int(bestQ*10))
	} else {
		best.Reason = "üÜï –ù–æ–≤—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω (–Ω–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏)"
	}
	return best
}

// Learn –æ–±–Ω–æ–≤–ª—è–µ—Ç Q-—Ç–∞–±–ª–∏—Ü—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–≥—Ä–∞–¥—ã
func (a *Agent) Learn(s State, action Action, reward Reward, nextS State) {
	a.mu.Lock()
	defer a.mu.Unlock()

	key := stateKey(s)
	nextKey := stateKey(nextS)

	// –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
	if a.qTable[key] == nil {
		a.qTable[key] = make(map[string]float64)
	}
	if a.qTable[nextKey] == nil {
		a.qTable[nextKey] = make(map[string]float64)
	}

	// Q-learning update: Q(s,a) += lr * (r + Œ≥*max_a' Q(s',a') - Q(s,a))
	currentQ := a.qTable[key][action.ID]
	
	// Max Q for next state
	maxNextQ := -math.MaxFloat64
	for _, q := range a.qTable[nextKey] {
		if q > maxNextQ {
			maxNextQ = q
		}
	}
	if maxNextQ == -math.MaxFloat64 {
		maxNextQ = 0
	}

	newQ := currentQ + a.learningRate*(float64(reward)+a.discount*maxNextQ-currentQ)
	a.qTable[key][action.ID] = newQ
}

// ExportExperience —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –æ–ø—ã—Ç –¥–ª—è gossip-–æ–±–º–µ–Ω–∞ (—Ç–æ–ª—å–∫–æ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
func (a *Agent) ExportExperience() []Experience {
	// –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏: –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, –Ω–µ —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
	return []Experience{}
}

// Experience ‚Äî –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–ø—ã—Ç –¥–ª—è –æ–±–º–µ–Ω–∞ –º–µ–∂–¥—É —É–∑–ª–∞–º–∏
type Experience struct {
	StateHash   string  // –•—ç—à —Å–æ—Å—Ç–æ—è–Ω–∏—è (–Ω–µ —Å–∞–º–∏ –¥–∞–Ω–Ω—ã–µ)
	ActionID    string
	AvgReward   float64
	Count       int
	Timestamp   time.Time
}
